{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Model2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMHKO1XGBKEhBJ5Xp/I2icQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anushka-code/Code-Smell-Classification/blob/main/Model2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Multimodal Deep Learning : Merging CNN & BiLSTM for Numerical and Textual Features"
      ],
      "metadata": {
        "id": "95X0DGgQMzaS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Code Smells Targetted: \n",
        "\n",
        "\n",
        "1.   Long Parameters List\n",
        "2.   Switch Statements\n"
      ],
      "metadata": {
        "id": "nH8lmuKQM7as"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Importing Libraries"
      ],
      "metadata": {
        "id": "wTJ9H70NNgRo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "jNZDUbFLLTWq"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import sklearn\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras.models import Functional, Model\n",
        "from keras.layers import Input, Convolution1D, MaxPooling1D, Flatten, Dense, concatenate\n",
        "from keras.utils import np_utils\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.layers import LSTM, Embedding, Bidirectional\n",
        "import imblearn\n",
        "from imblearn.over_sampling import SMOTE\n",
        "import nltk \n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "import re\n",
        "import gensim\n",
        "from gensim.models import Word2Vec\n",
        "from gensim.scripts.glove2word2vec import glove2word2vec"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Mounting Google Drive"
      ],
      "metadata": {
        "id": "Oem9ZwbuNh4P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "metadata": {
        "id": "dK6b3GWgNhR7"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Dataset Loader"
      ],
      "metadata": {
        "id": "V47ye9N4NwRe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def DataLoader(link, name_of_file):\n",
        "  id = link.split(\"/\")[-2]\n",
        "  downloaded = drive.CreateFile({'id':id}) \n",
        "  downloaded.GetContentFile(name_of_file)\n",
        "  dataframe = pd.read_csv(name_of_file)\n",
        "  return dataframe\n",
        "\n",
        "\n",
        "link1 = 'https://drive.google.com/file/d/1EfbAqgr7i9h4yFwEoU3igG34Gt48l6WT/view?usp=sharing'\n",
        "link2 = 'https://drive.google.com/file/d/1Ya1OMWsz1yyXAaZheIck-roX0M9UWiqg/view?usp=sharing'\n",
        "link6 = 'https://drive.google.com/file/d/1rLkJAwHkBAAkHMp2L1Y1AzuniIZdyB7x/view?usp=sharing'\n",
        "\n",
        "name1 = 'long_parameters_list_structural.csv'\n",
        "name2 = 'switch_statements_structural.csv'\n",
        "name6 = 'semantic_final.csv'\n",
        "\n",
        "df_lp = DataLoader(link1, name1)\n",
        "df_ss = DataLoader(link2, name2)\n",
        "df_semantic = DataLoader(link6,name6)"
      ],
      "metadata": {
        "id": "JNGd4c6vNr4v"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Data Pre-Processing of Structural Dataset\n"
      ],
      "metadata": {
        "id": "BgMKtYkcFa1p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def PrePro(last_column, dataframe):\n",
        "  dataframe.rename(columns = {last_column :'is_code_smell'}, inplace = True) #rename column\n",
        "  dataframe['is_code_smell'] = dataframe[\"is_code_smell\"].astype(int) #change boolean labels to int labels\n",
        "  Y_part = dataframe.iloc[:,-1:]\n",
        "  X_part = dataframe.iloc[:,:56]\n",
        "  X_part = X_part.replace(to_replace =[\"?\"], value = np.nan) #replace non existing values with null\n",
        "  X_part = X_part.astype(float) #change datatype of features of X as float\n",
        "  return X_part,Y_part\n",
        "\n",
        "X_lp, Y_lp = PrePro('is_long_parameters_list',df_lp)\n",
        "X_ss, Y_ss = PrePro('is_switch_statements',df_ss)"
      ],
      "metadata": {
        "id": "--dNJqY9NyxU"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def MeanforNaN(dataframe):   #function to fill null spaces with column mean \n",
        "  column_means = dataframe.mean()\n",
        "  dataframe = dataframe.fillna(column_means)\n",
        "  return dataframe\n",
        "\n",
        "X_lp = MeanforNaN(X_lp)\n",
        "X_ss = MeanforNaN(X_ss)"
      ],
      "metadata": {
        "id": "TQyHEDeIGNbM"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def ConCat(df1,df2): #concatenate code smell datasets\n",
        "  code_smells = [df1,df2]\n",
        "  joint = pd.concat(code_smells)\n",
        "  return joint\n",
        "\n",
        "X_train = ConCat(X_lp,X_ss)\n",
        "Y_train = ConCat(Y_lp,Y_ss)"
      ],
      "metadata": {
        "id": "11NCvNOHGSAQ"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def Normalize(dataframe): #apply MinMax normalisation to fit the values between 0 to 1\n",
        "  scaler = MinMaxScaler()\n",
        "  model = scaler.fit(dataframe)\n",
        "  scaled_data = model.transform(dataframe)\n",
        "  return scaled_data\n",
        "\n",
        "X_sample = Normalize(X_train)\n",
        "Y_sample = Y_train.to_numpy(dtype='int64', copy='True')"
      ],
      "metadata": {
        "id": "L6r_vcy0GVRk"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Synthetic Minority Oversampling Technique (SMOTE) Algo for Imbalanced Datasets"
      ],
      "metadata": {
        "id": "urNXFDPtGs0U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def Oversample(X_data,Y_data): # Using Smote obtain a 50-50 balanced dataset \n",
        "\n",
        "  sm = SMOTE(random_state = 2)\n",
        "  X_train_res, Y_train_res = sm.fit_resample(X_data, Y_data.ravel())\n",
        "  return X_train_res, Y_train_res\n",
        "\n",
        "X_new, Y_new = Oversample(X_sample,Y_sample)"
      ],
      "metadata": {
        "id": "XXU_4EBjGhPl"
      },
      "execution_count": 12,
      "outputs": []
    }
  ]
}