{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Model2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOB0KeAONUZ5/2guZGd8b+Q",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anushka-code/Code-Smell-Classification/blob/main/Model2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Multimodal Deep Learning : Merging CNN & BiLSTM for Numerical and Textual Features"
      ],
      "metadata": {
        "id": "95X0DGgQMzaS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Code Smells Targetted: \n",
        "\n",
        "\n",
        "1.   Long Parameters List\n",
        "2.   Switch Statements\n"
      ],
      "metadata": {
        "id": "nH8lmuKQM7as"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Importing Libraries"
      ],
      "metadata": {
        "id": "wTJ9H70NNgRo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "jNZDUbFLLTWq"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import sklearn\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras.models import Functional, Model\n",
        "from keras.layers import Input, Convolution1D, MaxPooling1D, Flatten, Dense, concatenate\n",
        "from keras.utils import np_utils\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.layers import LSTM, Embedding, Bidirectional\n",
        "import imblearn\n",
        "from imblearn.over_sampling import SMOTE\n",
        "import nltk \n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "import re\n",
        "import gensim\n",
        "from gensim.models import Word2Vec\n",
        "from gensim.scripts.glove2word2vec import glove2word2vec"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Mounting Google Drive"
      ],
      "metadata": {
        "id": "Oem9ZwbuNh4P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "metadata": {
        "id": "dK6b3GWgNhR7"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Dataset Loader"
      ],
      "metadata": {
        "id": "V47ye9N4NwRe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def DataLoader(link, name_of_file):\n",
        "  id = link.split(\"/\")[-2]\n",
        "  downloaded = drive.CreateFile({'id':id}) \n",
        "  downloaded.GetContentFile(name_of_file)\n",
        "  dataframe = pd.read_csv(name_of_file)\n",
        "  return dataframe\n",
        "\n",
        "\n",
        "link1 = 'https://drive.google.com/file/d/1EfbAqgr7i9h4yFwEoU3igG34Gt48l6WT/view?usp=sharing'\n",
        "link2 = 'https://drive.google.com/file/d/1Ya1OMWsz1yyXAaZheIck-roX0M9UWiqg/view?usp=sharing'\n",
        "link6 = 'https://drive.google.com/file/d/1rLkJAwHkBAAkHMp2L1Y1AzuniIZdyB7x/view?usp=sharing'\n",
        "\n",
        "name1 = 'long_parameters_list_structural.csv'\n",
        "name2 = 'switch_statements_structural.csv'\n",
        "name6 = 'semantic_final.csv'\n",
        "\n",
        "df_lp = DataLoader(link1, name1)\n",
        "df_ss = DataLoader(link2, name2)\n",
        "df_semantic = DataLoader(link6,name6)"
      ],
      "metadata": {
        "id": "JNGd4c6vNr4v"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Data Pre-Processing of Structural Dataset\n"
      ],
      "metadata": {
        "id": "BgMKtYkcFa1p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def PrePro(last_column, dataframe):\n",
        "  dataframe.rename(columns = {last_column :'is_code_smell'}, inplace = True) #rename column\n",
        "  dataframe['is_code_smell'] = dataframe[\"is_code_smell\"].astype(int) #change boolean labels to int labels\n",
        "  Y_part = dataframe.iloc[:,-1:]\n",
        "  X_part = dataframe.iloc[:,:56]\n",
        "  X_part = X_part.replace(to_replace =[\"?\"], value = np.nan) #replace non existing values with null\n",
        "  X_part = X_part.astype(float) #change datatype of features of X as float\n",
        "  return X_part,Y_part\n",
        "\n",
        "X_lp, Y_lp = PrePro('is_long_parameters_list',df_lp)\n",
        "X_ss, Y_ss = PrePro('is_switch_statements',df_ss)"
      ],
      "metadata": {
        "id": "--dNJqY9NyxU"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def MeanforNaN(dataframe):   #function to fill null spaces with column mean \n",
        "  column_means = dataframe.mean()\n",
        "  dataframe = dataframe.fillna(column_means)\n",
        "  return dataframe\n",
        "\n",
        "X_lp = MeanforNaN(X_lp)\n",
        "X_ss = MeanforNaN(X_ss)"
      ],
      "metadata": {
        "id": "TQyHEDeIGNbM"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def ConCat(df1,df2): #concatenate code smell datasets\n",
        "  code_smells = [df1,df2]\n",
        "  joint = pd.concat(code_smells)\n",
        "  return joint\n",
        "\n",
        "X_train = ConCat(X_lp,X_ss)\n",
        "Y_train = ConCat(Y_lp,Y_ss)"
      ],
      "metadata": {
        "id": "11NCvNOHGSAQ"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def Normalize(dataframe): #apply MinMax normalisation to fit the values between 0 to 1\n",
        "  scaler = MinMaxScaler()\n",
        "  model = scaler.fit(dataframe)\n",
        "  scaled_data = model.transform(dataframe)\n",
        "  return scaled_data\n",
        "\n",
        "X_sample = Normalize(X_train)\n",
        "Y_sample = Y_train.to_numpy(dtype='int64', copy='True')"
      ],
      "metadata": {
        "id": "L6r_vcy0GVRk"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Synthetic Minority Oversampling Technique (SMOTE) Algo for Imbalanced Datasets"
      ],
      "metadata": {
        "id": "urNXFDPtGs0U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def Oversample(X_data,Y_data): #Using Smote obtain a 50-50 balanced dataset \n",
        "\n",
        "  sm = SMOTE(random_state = 2)\n",
        "  X_train_res, Y_train_res = sm.fit_resample(X_data, Y_data.ravel())\n",
        "  return X_train_res, Y_train_res\n",
        "\n",
        "X_struct, Y_new = Oversample(X_sample,Y_sample)"
      ],
      "metadata": {
        "id": "XXU_4EBjGhPl"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Text Pre Processing"
      ],
      "metadata": {
        "id": "2r7C3Ea9HpwL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def PrePro2(dataframe):\n",
        "  X_part = dataframe.iloc[:,:-1]  \n",
        "  return X_part\n",
        "\n",
        "X_sem= PrePro2(df_semantic) #separate as X and Y columns"
      ],
      "metadata": {
        "id": "IUAOIuIEIgYO"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def ToList(dataframe, string): #converts pandas.core.series.Series to list of lists\n",
        "  new_column = dataframe[string].tolist()\n",
        "  return list(map(lambda x:[x], new_column))\n",
        "\n",
        "class_words = ToList(X_sem, 'class')\n",
        "method_words = ToList(X_sem, 'method')"
      ],
      "metadata": {
        "id": "XruwZQ9MIh4y"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Tokenization"
      ],
      "metadata": {
        "id": "29fhQJReLm9r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def TokenizeGroups(text): #tokenizes as per CamelCase RegEx and converts to lowercase\n",
        "  tokenizer = RegexpTokenizer('[a-zA-Z](?:[a-z]+|[A-Z]*(?=[A-Z]|$))') # RegEx pattern for CamelCase \n",
        "  useful_text = tokenizer.tokenize(text) \n",
        "  useful_text = [x.lower() for x in useful_text]\n",
        "  return useful_text"
      ],
      "metadata": {
        "id": "afm85O_zLpkT"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def LoopOver(list): #loops over all the samples to tokenize all the strings in each word group\n",
        "  for i in range(0,1146):\n",
        "    list[i] = TokenizeGroups(list[i][0])\n",
        "    i=i+1\n",
        "  return list\n",
        "\n",
        "class_words_sem = LoopOver(class_words)\n",
        "method_words_sem = LoopOver(method_words)"
      ],
      "metadata": {
        "id": "owrZWWIzLq8M"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def WordList(list1,list2): #concatenate the words of each project, package, class and method\n",
        "  final_list = []\n",
        "  for i in range(0,1146):\n",
        "    x = list1[i] + list2[i]\n",
        "    final_list.append(x)\n",
        "  res = [' '.join(ele) for ele in final_list]\n",
        "  return res\n",
        "\n",
        "word_groups = WordList(class_words_sem, method_words_sem)"
      ],
      "metadata": {
        "id": "N2st8FlZLw89"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def MaxNumWords(groupofwords):  \n",
        "  max = 0 # find the max number of words a sentence has in word groups\n",
        "  for ele in groupofwords:\n",
        "    res = len(ele.split())\n",
        "    if res > max:\n",
        "      max = res\n",
        "  return max\n",
        "\n",
        "max_words = MaxNumWords(word_groups)"
      ],
      "metadata": {
        "id": "f9KL8RkqSp_G"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "token = Tokenizer()\n",
        "token.fit_on_texts(word_groups)\n",
        "vocab_size = len(token.word_index) + 1\n",
        "print(vocab_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YOAp4vmC9Aj0",
        "outputId": "72063cd3-da16-42f2-d873-386299d1dc66"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1300\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "encoded_text = token.texts_to_sequences(word_groups)"
      ],
      "metadata": {
        "id": "MC3QmsTO9fSB"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_seman = pad_sequences(encoded_text, maxlen = 28, padding = 'pre')\n",
        "print(X_seman)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tx93NJDw9i-w",
        "outputId": "e8a6a885-0b16-4d67-e4d9-254f07269ec4"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[  0   0   0 ... 841  12 239]\n",
            " [  0   0   0 ... 511  46 165]\n",
            " [  0   0   0 ...  51 166 106]\n",
            " ...\n",
            " [  0   0   0 ...   1   1   1]\n",
            " [  0   0   0 ...   1   1   2]\n",
            " [  0   0   0 ...   2   2   2]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Glove Vectors using Gensim"
      ],
      "metadata": {
        "id": "OLnayh6fUVi0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim.downloader as api\n",
        "glove_gensim  = api.load('glove-wiki-gigaword-100') #100 dimensional"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WkmWAqGuUYVO",
        "outputId": "e632dbae-2f57-4ded-c6eb-4a5fddb7d24a"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[==================================================] 100.0% 128.1/128.1MB downloaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# gensim_weight_matrix = np.zeros((vocab_size ,vector_size))\n",
        "\n",
        "# def GloveVectorization(groupofwords, max_length, vector_size=100, gensim_matrix = ):\n",
        "#   token = Tokenizer()\n",
        "#   token.fit_on_texts(word_groups) \n",
        "#   vocab_size = len(token.word_index) + 1\n",
        "#   encoded_text = token.texts_to_sequences(word_groups)\n",
        "#   X = pad_sequences(encoded_text, maxlen = max_length, padding = 'pre') \n",
        "#   for word, index in token.word_index.items():\n",
        "#     if index < vocab_size: \n",
        "#         if word in glove_gensim.wv.vocab:\n",
        "#             gensim_weight_matrix[index] = glove_gensim[word]\n",
        "#         else:\n",
        "#             gensim_weight_matrix[index] = np.zeros(100)\n",
        "  \n",
        "#   return X, gensim_weight_matrix"
      ],
      "metadata": {
        "id": "3JbKeqLDPJ2Z"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vector_size = 100 \n",
        "gensim_weight_matrix = np.zeros((1300 ,vector_size)) \n",
        "gensim_weight_matrix.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EXwhbJVqUhq5",
        "outputId": "c253cf70-bfff-4cea-cc89-1f69832ef3a1"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1300, 100)"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for word, index in token.word_index.items():\n",
        "    if index < vocab_size: \n",
        "        if word in glove_gensim.wv.vocab:\n",
        "            gensim_weight_matrix[index] = glove_gensim[word]\n",
        "        else:\n",
        "            gensim_weight_matrix[index] = np.zeros(100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LEtOXh39Ui34",
        "outputId": "41a09a1f-4f09-4d84-c06b-3deeaea1cfc5"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:3: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model 2 - Multimodal of CNN & BiLSTM"
      ],
      "metadata": {
        "id": "UwM4u6fiDVF6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "height, width, depth = 1146, 56, 1\n",
        "input_shape=(width,depth)\n",
        "input_struct = Input(shape=input_shape)\n",
        "layer_1 = Convolution1D(filters=32, kernel_size=3, activation='relu')(input_struct)\n",
        "layer_2 = Convolution1D(filters=64, kernel_size=3, activation='relu')(layer_1)\n",
        "layer_3 = Convolution1D(filters=64, kernel_size=3, activation='relu')(layer_2)\n",
        "flatten_cnn = Flatten()(layer_3)\n",
        "\n",
        "model_left = Model(input_struct, flatten_cnn)"
      ],
      "metadata": {
        "id": "jK0JVgymDaIH"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EMBEDDING_DIM = 100\n",
        "input_sem = Input(shape=(28))\n",
        "embedding_layer = Embedding(input_dim = vocab_size, output_dim = EMBEDDING_DIM, input_length= X_seman.shape[1],\n",
        "                    weights = [gensim_weight_matrix],trainable = False)(input_sem)\n",
        "layer_1 = Bidirectional(LSTM(100,return_sequences=True))(embedding_layer)\n",
        "layer_2 = Bidirectional(LSTM(200,return_sequences=True))(layer_1)\n",
        "layer_3 = Bidirectional(LSTM(100,return_sequences=False))(layer_2)\n",
        "flatten_bilstm = Flatten()(layer_3)\n",
        "\n",
        "model_right = Model(input_sem, flatten_bilstm)"
      ],
      "metadata": {
        "id": "O1Dbu9plVRiY"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merge_layer = concatenate([flatten_cnn, flatten_bilstm])\n",
        "dense_1 = Dense(500, activation='relu')(merge_layer)\n",
        "dense_2 = Dense(100, activation='relu')(dense_1)\n",
        "dense3 = Dense(20, activation='relu')(dense_2)\n",
        "dense_final = Dense(1, activation='sigmoid')(dense3)"
      ],
      "metadata": {
        "id": "fzF1YeHjVe7j"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merged_model = Model([input_struct, input_sem], dense_final)"
      ],
      "metadata": {
        "id": "Ys5L5k_TVhs8"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merged_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "5XeemdJ_VoCo"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merged_model.fit([X_struct, X_seman], y=Y_new, batch_size=100, epochs=10, validation_split = 0.2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ryWyL7J7Vrur",
        "outputId": "eeb0018a-3616-4c43-d735-3ed77a071b0c"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "10/10 [==============================] - 28s 431ms/step - loss: 0.6190 - accuracy: 0.6834 - val_loss: 0.1869 - val_accuracy: 0.9391\n",
            "Epoch 2/10\n",
            "10/10 [==============================] - 1s 88ms/step - loss: 0.3991 - accuracy: 0.8122 - val_loss: 0.1382 - val_accuracy: 0.9348\n",
            "Epoch 3/10\n",
            "10/10 [==============================] - 1s 81ms/step - loss: 0.3326 - accuracy: 0.8614 - val_loss: 0.1968 - val_accuracy: 0.9130\n",
            "Epoch 4/10\n",
            "10/10 [==============================] - 1s 80ms/step - loss: 0.3044 - accuracy: 0.8548 - val_loss: 0.3927 - val_accuracy: 0.8478\n",
            "Epoch 5/10\n",
            "10/10 [==============================] - 1s 77ms/step - loss: 0.2700 - accuracy: 0.8777 - val_loss: 0.1436 - val_accuracy: 0.9391\n",
            "Epoch 6/10\n",
            "10/10 [==============================] - 1s 74ms/step - loss: 0.2337 - accuracy: 0.9028 - val_loss: 0.1533 - val_accuracy: 0.9348\n",
            "Epoch 7/10\n",
            "10/10 [==============================] - 1s 74ms/step - loss: 0.2192 - accuracy: 0.9039 - val_loss: 0.1905 - val_accuracy: 0.9261\n",
            "Epoch 8/10\n",
            "10/10 [==============================] - 1s 73ms/step - loss: 0.1981 - accuracy: 0.9225 - val_loss: 0.2717 - val_accuracy: 0.9174\n",
            "Epoch 9/10\n",
            "10/10 [==============================] - 1s 95ms/step - loss: 0.1846 - accuracy: 0.9236 - val_loss: 0.3312 - val_accuracy: 0.9043\n",
            "Epoch 10/10\n",
            "10/10 [==============================] - 1s 72ms/step - loss: 0.1671 - accuracy: 0.9312 - val_loss: 0.3793 - val_accuracy: 0.9000\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f169e5e8b50>"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    }
  ]
}